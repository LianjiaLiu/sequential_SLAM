{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T21:59:46.670325Z",
     "start_time": "2024-03-14T21:59:43.626325Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# Enable autoreloading if import packages are changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set up python path \n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\") #we assume the default setting will be cpu for better testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1080, 1920, 3)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([])\n",
      "tensor([], dtype=torch.int64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 146\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isDebug: \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(imgs)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(imgs)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 146\u001b[0m generate_SIFT_corres(imgs, target_object,num_of_match_threshold, match_threshold_SIFT)\n",
      "Cell \u001b[1;32mIn[4], line 110\u001b[0m, in \u001b[0;36mgenerate_SIFT_corres\u001b[1;34m(imgs, obj_name, num_of_match_threshold, match_threhold)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03mdetails abt two threshold are in feature_matching_file function\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m kps, descs \u001b[38;5;241m=\u001b[39m extract_features(imgs, SIFT)\n\u001b[1;32m--> 110\u001b[0m feature_matching_file(kps,descs,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSIFT\u001b[39m\u001b[38;5;124m'\u001b[39m,obj_name, num_of_match_threshold, match_threhold)\n",
      "Cell \u001b[1;32mIn[4], line 75\u001b[0m, in \u001b[0;36mfeature_matching_file\u001b[1;34m(keypointxy_for_all, descriptor_for_all, method_name, obj_name, num_of_match_threshold, match_score_threshold)\u001b[0m\n\u001b[0;32m     72\u001b[0m matches_for_one_img \u001b[38;5;241m=\u001b[39m []    \n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m     matches_idx \u001b[38;5;241m=\u001b[39m matcher\u001b[38;5;241m.\u001b[39mmatch(\n\u001b[0;32m     76\u001b[0m         descriptors1\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(descriptor_for_all[i], device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m     77\u001b[0m         descriptors2\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(descriptor_for_all[j], device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m     78\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     79\u001b[0m         dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhamming\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     80\u001b[0m         ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m,\n\u001b[0;32m     81\u001b[0m         threshold\u001b[38;5;241m=\u001b[39mmatch_score_threshold\n\u001b[0;32m     82\u001b[0m     )\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in matching descriptors for image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mrina\\OneDrive\\Desktop\\3dproj\\stage2_v2\\stage2\\src\\match.py:63\u001b[0m, in \u001b[0;36mmatch\u001b[1;34m(descriptors1, descriptors2, device, dist, threshold, ratio)\u001b[0m\n\u001b[0;32m     61\u001b[0m     distance_matrix \u001b[38;5;241m=\u001b[39m euclidean_distance_squared(descriptors1, descriptors2)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m     distance_matrix \u001b[38;5;241m=\u001b[39m hamming_distance(descriptors1, descriptors2)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     65\u001b[0m matches \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     66\u001b[0m distances \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\mrina\\OneDrive\\Desktop\\3dproj\\stage2_v2\\stage2\\src\\match.py:38\u001b[0m, in \u001b[0;36mhamming_distance\u001b[1;34m(descriptors1, descriptors2)\u001b[0m\n\u001b[0;32m     35\u001b[0m xor_result \u001b[38;5;241m=\u001b[39m descriptors1\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m^\u001b[39m descriptors2\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Count the number of set bits\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m hamming_dist \u001b[38;5;241m=\u001b[39m xor_result\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hamming_dist\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import src.match as matcher\n",
    "from skimage.feature import plot_matches\n",
    "import os\n",
    "\n",
    "SIFT  = 0\n",
    "ORB   = 1\n",
    "BRIEF = 2 \n",
    "KEYPOINT_NUM_THREHOLD = 10\n",
    "NEED_COLOR = False\n",
    "isDebug =  False #also read out the file to plot the correspondences, just for two chess imgs\n",
    "\n",
    "def extract_features(imgs, method_name = SIFT):\n",
    "    \"\"\"\n",
    "    output:\n",
    "    keypointxy_for_all: shape(list) = [images, num_of_keypoint, x, y]\n",
    "    descriptor_foa_all: shape(list) = [images, num_of_keypoint, descriptor]\n",
    "    colors_for_all: shape(list) = [images, num_of_keypoint, color]\n",
    "    \"\"\"\n",
    "    if method_name == SIFT: extractor = cv2.SIFT_create() #default (int nfeature=0, int nOctaveLayers=3, double contrastThreshold = 0.04)\n",
    "    elif method_name == ORB: extractor = cv2.ORB_create()\n",
    "    elif method_name == BRIEF: \n",
    "        fast = cv2.FastFeatureDetector_create() \n",
    "        brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "    keypointxy_for_all = []\n",
    "    descriptor_for_all = []\n",
    "    colors_for_all = [] #for later shading, default is no use of this\n",
    "    for img in imgs:\n",
    "\n",
    "        if img is None:\n",
    "            continue\n",
    "        if method_name != SIFT and method_name!= ORB:\n",
    "            keypoints = fast.detect(img, None)\n",
    "            keypoints, descriptors = brief.compute(img,keypoints)\n",
    "        else:    \n",
    "            keypoints, descriptors = extractor.detectAndCompute(cv2.cvtColor(img,cv2.COLOR_BGR2GRAY),None)\n",
    "        \n",
    "        if len(keypoints) <= KEYPOINT_NUM_THREHOLD: continue\n",
    "            \n",
    "        # print(descriptors.shape)\n",
    "\n",
    "        pt = np.array([kp.pt for kp in keypoints])\n",
    "        # print(pt.shape)\n",
    "        keypointxy_for_all.append(pt)\n",
    "        descriptor_for_all.append(descriptors)\n",
    "\n",
    "        if NEED_COLOR:\n",
    "            colors = np.zeros((len(keypoints),3))\n",
    "            for i, kp in enumerate(keypoints):\n",
    "                p = kp.pt\n",
    "                colors[i] = img[int(p[1])][int(p[0])]\n",
    "            colors_for_all.append(colors)\n",
    "    \n",
    "    # for_all are all list instead of numpy array, due to diff dimensions of diff imgs\n",
    "    if NEED_COLOR:\n",
    "        return np.array(keypointxy_for_all), np.array(descriptor_for_all), np.array(colors_for_all)\n",
    "    else:\n",
    "        return keypointxy_for_all, descriptor_for_all\n",
    "    \n",
    "def feature_matching_file(keypointxy_for_all, descriptor_for_all, method_name, obj_name, num_of_match_threshold=10, match_score_threshold=70):\n",
    "    \"\"\"\n",
    "    Function: iterate over all images and generate the matching file \n",
    "    NOTE: might need to fine-tune the threshold to generate both sufficient and good matches\n",
    "    \n",
    "    Outputs:\n",
    "    method_name:             SIFT, BRIEF, SURF, etc.\n",
    "    num_of_match_threshold:  if num_of_match < this, skip generating correspondence for these two images\n",
    "    match_score_threshold:   if match_score < this, skip the match\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(len(descriptor_for_all)):\n",
    "        for j in range(i + 1, len(descriptor_for_all)):\n",
    "            matches_for_one_img = []    \n",
    "\n",
    "            try:\n",
    "                matches_idx = matcher.match(\n",
    "                    descriptors1=torch.tensor(descriptor_for_all[i], device=device, dtype=torch.float32),\n",
    "                    descriptors2=torch.tensor(descriptor_for_all[j], device=device, dtype=torch.float32),\n",
    "                    device=\"cpu\",\n",
    "                    dist='hamming',\n",
    "                    ratio=0.99,\n",
    "                    threshold=match_score_threshold\n",
    "                ).detach().cpu().numpy()\n",
    "            except Exception as e:\n",
    "                print(f\"Error in matching descriptors for image {i} and image {j}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if len(matches_idx) < num_of_match_threshold:\n",
    "                continue\n",
    "\n",
    "            print(f'img{i} has {len(matches_idx)} matches with img{j}')\n",
    "\n",
    "            for k in range(len(matches_idx)):\n",
    "                single_match = np.array([keypointxy_for_all[i][matches_idx[k][0]], keypointxy_for_all[j][matches_idx[k][1]]]).flatten()\n",
    "                # assert single_match.shape==(4,)\n",
    "                matches_for_one_img.append(single_match)\n",
    "\n",
    "            try:\n",
    "                np.savetxt(f\"./corres/{obj_name}/{method_name}_corres/{i}_{j}.txt\", matches_for_one_img)\n",
    "                print(f'./corres/{obj_name}/{method_name}_corres/{i}_{j}.txt is generated')\n",
    "            except Exception as e:\n",
    "                print(f\"Error in saving correspondence file for image {i} and image {j}: {e}\")\n",
    "        \n",
    "        break\n",
    "\n",
    "def generate_SIFT_corres(imgs, obj_name, num_of_match_threshold=10, match_threhold=70):\n",
    "    \"\"\"\n",
    "    details abt two threshold are in feature_matching_file function\n",
    "    \"\"\"\n",
    "    kps, descs = extract_features(imgs, SIFT)\n",
    "    feature_matching_file(kps,descs,'SIFT',obj_name, num_of_match_threshold, match_threhold)\n",
    "\n",
    "def generate_BRIEF_corres(imgs, obj_name, num_of_match_threshold=10, match_threhold=10):\n",
    "    \"\"\"\n",
    "    details abt two threshold are in feature_matching_file function\n",
    "    \"\"\"\n",
    "    kps, descs = extract_features(imgs, BRIEF)\n",
    "    feature_matching_file(kps,descs,'BRIEF',obj_name, num_of_match_threshold, match_threhold)\n",
    "\n",
    "def generate_ORB_corres(imgs, obj_name, num_of_match_threshold=10, match_threhold=20):\n",
    "    \"\"\"\n",
    "    details abt two threshold are in feature_matching_file function\n",
    "    \"\"\"\n",
    "    kps, descs = extract_features(imgs, ORB)\n",
    "    feature_matching_file(kps,descs,'ORB',obj_name, num_of_match_threshold, match_threhold)\n",
    "\n",
    "\n",
    "target_object = 'milk' #NOTE: change this to generate matches for different objects\n",
    "match_threshold_SIFT = 70\n",
    "match_threshold_BRIEF = 10\n",
    "match_threshold_ORB = 20\n",
    "num_of_match_threshold = 10\n",
    "\n",
    "path = \"C:/Users/mrina/OneDrive/Desktop/3dproj/stage2_v2/stage2/data/{}/images/\".format(target_object) #NOTE: please change the path or put the image files accordingly\n",
    "files = os.listdir(path)\n",
    "# print(files)\n",
    "imgs = []\n",
    "for file in files:\n",
    "    if not os.path.isdir(file):\n",
    "        # print(path+\"/\"+file)\n",
    "        img = cv2.imread(path+\"/\"+file);\n",
    "        imgs.append(img)\n",
    "if isDebug: assert len(imgs)==2\n",
    "print(np.array(imgs).shape)\n",
    "\n",
    "\n",
    "generate_SIFT_corres(imgs, target_object,num_of_match_threshold, match_threshold_SIFT)\n",
    "# generate_ORB_corres(imgs, target_object,num_of_match_threshold, match_threshold_ORB)\n",
    "# generate_BRIEF_corres(imgs, target_object,num_of_match_threshold, match_threshold_BRIEF)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### only for combined matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'xfeatures2d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 87\u001b[0m\n\u001b[0;32m     82\u001b[0m     descs_combined \u001b[38;5;241m=\u001b[39m [descs_SIFT, descs_ORB, descs_BRIEF]\n\u001b[0;32m     84\u001b[0m     feature_matching_file_combined( kps_combined, descs_combined, obj_name, num_of_match_threshold, match_threshold_SIFT )\n\u001b[1;32m---> 87\u001b[0m generate_combined_corres(imgs, target_object)\n",
      "Cell \u001b[1;32mIn[13], line 78\u001b[0m, in \u001b[0;36mgenerate_combined_corres\u001b[1;34m(imgs, obj_name, num_of_match_threshold)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_combined_corres\u001b[39m(imgs, obj_name, num_of_match_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    details abt two threshold are in feature_matching_file function\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     kps_SIFT,  descs_SIFT  \u001b[38;5;241m=\u001b[39m extract_features(imgs, SIFT)\n\u001b[0;32m     79\u001b[0m     kps_ORB,   descs_ORB   \u001b[38;5;241m=\u001b[39m extract_features(imgs, ORB)\n\u001b[0;32m     80\u001b[0m     kps_BRIEF, descs_BRIEF \u001b[38;5;241m=\u001b[39m extract_features(imgs, BRIEF)\n",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(imgs, method_name)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(imgs, method_name \u001b[38;5;241m=\u001b[39m SIFT):\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    output:\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    keypointxy_for_all: shape(list) = [images, num_of_keypoint, x, y]\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    descriptor_foa_all: shape(list) = [images, num_of_keypoint, descriptor]\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    colors_for_all: shape(list) = [images, num_of_keypoint, color]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method_name \u001b[38;5;241m==\u001b[39m SIFT: extractor \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mxfeatures2d\u001b[38;5;241m.\u001b[39mSIFT_create() \u001b[38;5;66;03m#default (int nfeature=0, int nOctaveLayers=3, double contrastThreshold = 0.04)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m method_name \u001b[38;5;241m==\u001b[39m ORB: extractor \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mORB_create()\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m method_name \u001b[38;5;241m==\u001b[39m BRIEF: \n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'xfeatures2d'"
     ]
    }
   ],
   "source": [
    "\n",
    "def feature_matching_file_combined(keypointxy_for_all, descriptor_for_all,  obj_name, num_of_match_threshold = 16, match_score_threshold_SIFT =70, match_score_threshold_ORB=20, match_score_threshold_BRIEF=10):\n",
    "    \"\"\"\n",
    "    function: iterate over all images and generate the matching file \n",
    "    NOTE: might need to finetune the threhold to generate both sufficient and good matches\n",
    "\n",
    "    input:\n",
    "    keypointxy_for_all: shape(list) = [3,images, num_of_keypoint, x, y]\n",
    "    descriptor_foa_all: shape(list) = [3,images, num_of_keypoint, descriptor]\n",
    "\n",
    "    outputs:\n",
    "    method_name:            SIFT, BRIEF, SURF and etc.\n",
    "    num_of_match_threshold: if num_of_match < this, skip generating correspondence for these two images\n",
    "    match_score_threshold:  if match_score < this, skip the match\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(descriptor_for_all)-1):        \n",
    "        for j in range(i+1, len(descriptor_for_all)):\n",
    "            \n",
    "\n",
    "            matches_idx_SIFT = matcher.match(\n",
    "                                        descriptors1 = torch.tensor(descriptor_for_all[SIFT][i], device = device,dtype = torch.float32),\n",
    "                                        descriptors2 = torch.tensor(descriptor_for_all[SIFT][j], device = device,dtype = torch.float32),\n",
    "                                        device=\"cpu\",\n",
    "                                        dist='hamming',\n",
    "                                        ratio=0.95,\n",
    "                                        threshold = match_score_threshold_SIFT,\n",
    "                                        isdebug = False\n",
    "                                    ).detach().cpu().numpy()\n",
    "            matches_idx_ORB = matcher.match(\n",
    "                                        descriptors1 = torch.tensor(descriptor_for_all[ORB][i], device = device,dtype = torch.float32),\n",
    "                                        descriptors2 = torch.tensor(descriptor_for_all[ORB][j], device = device,dtype = torch.float32),\n",
    "                                        device=\"cpu\",\n",
    "                                        dist='hamming',\n",
    "                                        ratio=0.95,\n",
    "                                        threshold = match_score_threshold_ORB,\n",
    "                                        isdebug = False\n",
    "                                    ).detach().cpu().numpy()\n",
    "            matches_idx_BRIEF = matcher.match(\n",
    "                                        descriptors1 = torch.tensor(descriptor_for_all[BRIEF][i], device = device,dtype = torch.float32),\n",
    "                                        descriptors2 = torch.tensor(descriptor_for_all[BRIEF][j], device = device,dtype = torch.float32),\n",
    "                                        device=\"cpu\",\n",
    "                                        dist='hamming',\n",
    "                                        ratio=0.95,\n",
    "                                        threshold = match_score_threshold_BRIEF,\n",
    "                                        isdebug = False\n",
    "                                    ).detach().cpu().numpy()\n",
    "            \n",
    "            num_of_match = len(matches_idx_ORB)+len(matches_idx_BRIEF)+len(matches_idx_SIFT)\n",
    "            if(num_of_match < num_of_match_threshold): continue\n",
    "\n",
    "            print('img{} has {} matches with img{}'.format(i,num_of_match,j))\n",
    "            print('./corres/{}/combined_corres/{}_{}.txt is generated'.format(obj_name, i,j))\n",
    "\n",
    "            matches_for_one_img = []\n",
    "            matches_SIFT  = []\n",
    "            matches_ORB   = []\n",
    "            matches_BRIEF = []\n",
    "            \n",
    "            for k in range(get_max_matches_num(len(matches_idx_SIFT),len(matches_idx_BRIEF),len(matches_idx_ORB) )):\n",
    "\n",
    "                single_match = np.array([keypointxy_for_all[i][matches_idx[k][0]], keypointxy_for_all[j][matches_idx[k][1]]]).flatten()\n",
    "                \n",
    "                assert single_match.shape==(4,)\n",
    "                matches_for_one_img.append(single_match)\n",
    "\n",
    "    return 0\n",
    "\n",
    "def get_max_matches_num(x,y,z):\n",
    "    if  x >=y and x>=z: return x\n",
    "    elif y>=x and y>=z: return y \n",
    "    else:               return z \n",
    "\n",
    "def generate_combined_corres(imgs, obj_name, num_of_match_threshold = 10):\n",
    "    \"\"\"\n",
    "    details abt two threshold are in feature_matching_file function\n",
    "    \"\"\"\n",
    "\n",
    "    kps_SIFT,  descs_SIFT  = extract_features(imgs, SIFT)\n",
    "    kps_ORB,   descs_ORB   = extract_features(imgs, ORB)\n",
    "    kps_BRIEF, descs_BRIEF = extract_features(imgs, BRIEF)\n",
    "    kps_combined   = [  kps_SIFT,   kps_ORB,   kps_BRIEF]\n",
    "    descs_combined = [descs_SIFT, descs_ORB, descs_BRIEF]\n",
    "    \n",
    "    feature_matching_file_combined( kps_combined, descs_combined, obj_name, num_of_match_threshold, match_threshold_SIFT )\n",
    "\n",
    "\n",
    "generate_combined_corres(imgs, target_object)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splicing_methods(num_of_imgs):\n",
    "    for i in range(num_of_imgs-1):\n",
    "        for j in range(i+1,num_of_imgs):\n",
    "            match1 = np.array(np.loadtxt('./'))\n",
    "files = os.listdir(path)\n",
    "splicing_methods('SIFT', 'BRIEF', 'ORB',len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for debugging\n",
    "# checking if the plots are the same after reading from files, please correct the path to corresponding methods\n",
    "\n",
    "if isDebug:\n",
    "    match = np.array(np.loadtxt('./SIFT_corres/0_1.txt')) \n",
    "    print(match.shape)\n",
    "    fig, axs = plt.subplots(figsize=(30.0, 20.0))\n",
    "    k = 10\n",
    "    idx = np.array([[i,i] for i in range(10)])\n",
    "    plot_matches(\n",
    "        axs,\n",
    "        gray1,\n",
    "        gray2,\n",
    "        match[:,0:2],\n",
    "        match[:,2:4],\n",
    "        idx,\n",
    "        alignment=\"horizontal\",\n",
    "        only_matches=True,\n",
    "    )\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
